# self_attention_and_transformer
here are my slef_attention and transformer implementations.
